{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory you want to use\n",
    "directory = r\"C:\\Users\\rex\\Desktop\\水位預測\\1\\彰雲橋\"\n",
    "\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through all .csv files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        data = pd.read_csv(filepath)\n",
    "        \n",
    "        # Convert datetime to datetime format and split it into Date and Time\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "        data['Date'] = data['datetime'].dt.date\n",
    "        data['Time'] = data['datetime'].dt.time\n",
    "        data = data.drop(columns=['datetime'])\n",
    "        \n",
    "        # Append the data to the combined_data dataframe\n",
    "        combined_data = pd.concat([combined_data, data])\n",
    "\n",
    "# Create a complete datetime range\n",
    "complete_datetime_range = pd.date_range(start=combined_data['Date'].min(), end=combined_data['Date'].max(), freq='H')\n",
    "\n",
    "# Create a complete dataframe with all datetimes\n",
    "complete_data = pd.DataFrame()\n",
    "complete_data['datetime'] = complete_datetime_range\n",
    "complete_data['Date'] = complete_data['datetime'].dt.date\n",
    "complete_data['Time'] = complete_data['datetime'].dt.time\n",
    "\n",
    "# Merge the original data with the complete data\n",
    "complete_data = pd.merge(complete_data, combined_data, on=['Date', 'Time'], how='left')\n",
    "\n",
    "# If value is NaN after the merge, it means it was a missing datetime, keep it as NaN\n",
    "complete_data['19335 Rainfall'] = complete_data['19335 Rainfall']\n",
    "\n",
    "# Remove the temporary datetime column\n",
    "complete_data = complete_data.drop(columns=['datetime'])\n",
    "\n",
    "# Write the result to a .csv file\n",
    "complete_data.to_csv(r\"C:\\Users\\rex\\Desktop\\水位預測\\1\\彰雲橋\\output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', '19774 Rainfall'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file you want to use\n",
    "file = r\"C:\\Users\\rex\\Desktop\\水位預測\\1\\名竹大橋\\19774_2020.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Print column names\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Date      Time  19774 Rainfall\n",
      "Datetime                                                 \n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.044878\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.044878\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.083883\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.083883\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.044878\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.044878\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.158432\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.158432\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.158432\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.158432\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.044878\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.044878\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.083883\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.083883\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the original df2 to get the duplicate values\n",
    "df2_original = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\名竹大橋\\19774_整理.csv')\n",
    "df2_original['Datetime'] = pd.to_datetime(df2_original['Date'].astype(str) + ' ' + df2_original['Time'])\n",
    "df2_original.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Find the duplicate Datetime values\n",
    "duplicates_df2 = df2_original[df2_original.index.duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicate values\n",
    "print(duplicates_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Date      Time  20016 Rainfall\n",
      "Datetime                                                 \n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.180962\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.180962\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.065908\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.065908\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.000000\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.000000\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.069038\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.069038\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.000000\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.000000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the original df2 to get the duplicate values\n",
    "df2_original = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\西濱大橋\\20016_整理.csv')\n",
    "df2_original['Datetime'] = pd.to_datetime(df2_original['Date'].astype(str) + ' ' + df2_original['Time'])\n",
    "df2_original.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Find the duplicate Datetime values\n",
    "duplicates_df2 = df2_original[df2_original.index.duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicate values\n",
    "print(duplicates_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Date      Time  19606 Rainfall\n",
      "Datetime                                                 \n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.000000\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.000000\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.057345\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.057345\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.119816\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.119816\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.000000\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.000000\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.395676\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.395676\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.000000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the original df2 to get the duplicate values\n",
    "df2_original = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\溪州大橋\\19606_整理.csv')\n",
    "df2_original['Datetime'] = pd.to_datetime(df2_original['Date'].astype(str) + ' ' + df2_original['Time'])\n",
    "df2_original.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Find the duplicate Datetime values\n",
    "duplicates_df2 = df2_original[df2_original.index.duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicate values\n",
    "print(duplicates_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Date      Time  19335 Rainfall\n",
      "Datetime                                                 \n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-01 10:00:00  2022-11-01  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 10:00:00  2022-11-20  10:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 12:00:00  2022-11-20  12:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-20 13:00:00  2022-11-20  13:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 03:00:00  2022-11-21  03:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-21 04:00:00  2022-11-21  04:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 08:00:00  2022-11-23  08:00:00        0.000000\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.221698\n",
      "2022-11-23 09:00:00  2022-11-23  09:00:00        0.221698\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 10:00:00  2022-11-23  10:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 11:00:00  2022-11-23  11:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 12:00:00  2022-11-23  12:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 13:00:00  2022-11-23  13:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 14:00:00  2022-11-23  14:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 15:00:00  2022-11-23  15:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 16:00:00  2022-11-23  16:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 17:00:00  2022-11-23  17:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 18:00:00  2022-11-23  18:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 19:00:00  2022-11-23  19:00:00        0.000000\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.235493\n",
      "2022-11-23 20:00:00  2022-11-23  20:00:00        0.235493\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 21:00:00  2022-11-23  21:00:00        0.000000\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.000000\n",
      "2022-11-23 22:00:00  2022-11-23  22:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-23 23:00:00  2022-11-23  23:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 00:00:00  2022-11-24  00:00:00        0.000000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.250000\n",
      "2022-11-24 01:00:00  2022-11-24  01:00:00        0.250000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 02:00:00  2022-11-24  02:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 03:00:00  2022-11-24  03:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 04:00:00  2022-11-24  04:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 05:00:00  2022-11-24  05:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 06:00:00  2022-11-24  06:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n",
      "2022-11-24 07:00:00  2022-11-24  07:00:00        0.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the original df2 to get the duplicate values\n",
    "df2_original = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\彰雲橋\\19335_整理.csv')\n",
    "df2_original['Datetime'] = pd.to_datetime(df2_original['Date'].astype(str) + ' ' + df2_original['Time'])\n",
    "df2_original.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Find the duplicate Datetime values\n",
    "duplicates_df2 = df2_original[df2_original.index.duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicate values\n",
    "print(duplicates_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rex\\AppData\\Local\\Temp\\ipykernel_34592\\292711867.py:30: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df2 = df2.groupby(df2.index).mean()\n",
      "C:\\Users\\rex\\AppData\\Local\\Temp\\ipykernel_34592\\292711867.py:31: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df3 = df3.groupby(df3.index).mean()\n",
      "C:\\Users\\rex\\AppData\\Local\\Temp\\ipykernel_34592\\292711867.py:32: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df4 = df4.groupby(df4.index).mean()\n",
      "C:\\Users\\rex\\AppData\\Local\\Temp\\ipykernel_34592\\292711867.py:33: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df5 = df5.groupby(df5.index).mean()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\水位_整理.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\名竹大橋\\19774_整理.csv')\n",
    "df3 = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\西濱大橋\\20016_整理.csv')\n",
    "df4 = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\溪州大橋\\19606_整理.csv')\n",
    "df5 = pd.read_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\彰雲橋\\19335_整理.csv')\n",
    "# Ensure consistent date format\n",
    "df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "df4['Date'] = pd.to_datetime(df4['Date'])\n",
    "df5['Date'] = pd.to_datetime(df5['Date'])\n",
    "\n",
    "# Combine the Date and Time columns into a single datetime object and set it as the index\n",
    "df1['Datetime'] = pd.to_datetime(df1['Date'].astype(str) + ' ' + df1['Time'])\n",
    "df2['Datetime'] = pd.to_datetime(df2['Date'].astype(str) + ' ' + df2['Time'])\n",
    "df3['Datetime'] = pd.to_datetime(df3['Date'].astype(str) + ' ' + df2['Time'])\n",
    "df4['Datetime'] = pd.to_datetime(df4['Date'].astype(str) + ' ' + df2['Time'])\n",
    "df5['Datetime'] = pd.to_datetime(df5['Date'].astype(str) + ' ' + df2['Time'])\n",
    "\n",
    "df1.set_index('Datetime', inplace=True)\n",
    "df2.set_index('Datetime', inplace=True)\n",
    "df3.set_index('Datetime', inplace=True)\n",
    "df4.set_index('Datetime', inplace=True)\n",
    "df5.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Remove duplicates in df2 and take the mean of '19774 Rainfall' for duplicate Datetime values\n",
    "df2 = df2.groupby(df2.index).mean()\n",
    "df3 = df3.groupby(df3.index).mean()\n",
    "df4 = df4.groupby(df4.index).mean()\n",
    "df5 = df5.groupby(df5.index).mean()\n",
    "\n",
    "# Merge the two dataframes on the index (Datetime)\n",
    "df = pd.concat([df1, df2['19774 Rainfall'], df3['20016 Rainfall'], df4['19606 Rainfall'], df5['19335 Rainfall']], axis=1)\n",
    "\n",
    "# Fill missing values in the '19774 Rainfall' column with NaN\n",
    "df['19774 Rainfall'].fillna(value=pd.NA, inplace=True)\n",
    "df['20016 Rainfall'].fillna(value=pd.NA, inplace=True)\n",
    "df['19606 Rainfall'].fillna(value=pd.NA, inplace=True)\n",
    "df['19335 Rainfall'].fillna(value=pd.NA, inplace=True)\n",
    "\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "df.head()\n",
    "\n",
    "df.to_csv(r'C:\\Users\\rex\\Desktop\\水位預測\\1\\總整理.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 函數用於從檔案名稱解析日期和時間\n",
    "def parse_datetime_from_filename(filename):\n",
    "    datetime_str = \"_\".join(filename.split(\"_\")[-2:]).split(\".\")[0]  # 以 \"_\" 分割檔案名稱並取最後兩部分，然後去除檔案副檔名\n",
    "    return datetime.strptime(datetime_str, \"%Y%m%d_%H%M\")  # 解析日期和時間\n",
    "\n",
    "# 打開檔案並讀取所有行\n",
    "with open(r\"C:\\Users\\rex\\Desktop\\filelist.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 去除行尾的 '\\n'，解析日期和時間並進行排序\n",
    "datetimes = sorted(parse_datetime_from_filename(line.rstrip('\\n')) for line in lines)\n",
    "\n",
    "# 尋找缺失的日期和時間\n",
    "missing_datetimes = []\n",
    "for i in range(1, len(datetimes)):\n",
    "    expected_datetime = datetimes[i-1] + timedelta(minutes=10)\n",
    "    if datetimes[i] != expected_datetime:\n",
    "        missing_datetimes.append(expected_datetime)\n",
    "\n",
    "# 將缺失的日期和時間寫入一個新檔案\n",
    "with open(\"missing_datetimes.txt\", \"w\") as file:\n",
    "    for dt in missing_datetimes:\n",
    "        file.write(dt.strftime(\"%Y%m%d_%H%M\") + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連續時間缺值\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 函數用於從檔案名稱解析日期和時間\n",
    "def parse_datetime_from_filename(filename):\n",
    "    datetime_str = \"_\".join(filename.split(\"_\")[-2:]).split(\".\")[0]  # 以 \"_\" 分割檔案名稱並取最後兩部分，然後去除檔案副檔名\n",
    "    return datetime.strptime(datetime_str, \"%Y%m%d_%H%M\")  # 解析日期和時間\n",
    "\n",
    "# 打開檔案並讀取所有行\n",
    "with open(r\"C:\\Users\\rex\\Desktop\\filelist.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 去除行尾的 '\\n'，解析日期和時間並進行排序\n",
    "datetimes = sorted(parse_datetime_from_filename(line.rstrip('\\n')) for line in lines)\n",
    "\n",
    "# 尋找缺失的日期和時間\n",
    "missing_datetimes = []\n",
    "for i in range(1, len(datetimes)):\n",
    "    expected_datetime = datetimes[i-1] + timedelta(minutes=10)\n",
    "    while expected_datetime < datetimes[i]:\n",
    "        missing_datetimes.append(expected_datetime)\n",
    "        expected_datetime += timedelta(minutes=10)\n",
    "\n",
    "# 將缺失的日期和時間寫入一個新檔案\n",
    "with open(\"missing_datetimes.txt\", \"w\") as file:\n",
    "    for dt in missing_datetimes:\n",
    "        file.write(dt.strftime(\"%Y%m%d_%H%M\") + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連續時間缺值，該時間0000~0050都存在\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def find_fully_represented_hours(filename):\n",
    "    # 讀取數據\n",
    "    data = pd.read_csv(filename, header=None, names=['datetime'])\n",
    "\n",
    "    # 從 'datetime' 字串中提取日期、小時和分鐘資訊\n",
    "    data['date'] = data['datetime'].str[:8]\n",
    "    data['hour'] = data['datetime'].str[9:11]\n",
    "    data['minute'] = data['datetime'].str[11:]\n",
    "\n",
    "    # 根據日期和小時分組數據\n",
    "    grouped = data.groupby(['date', 'hour'])\n",
    "\n",
    "    # 初始化一個空列表，用於儲存所有小時內每10分鐘都有數據的時間點\n",
    "    fully_represented_hours = []\n",
    "\n",
    "    # 檢查每個分組中是否包含所有的分鐘（00, 10, 20, 30, 40, 50）\n",
    "    for name, group in grouped:\n",
    "        if set(group['minute']) == set(['00', '10', '20', '30', '40', '50']):\n",
    "            fully_represented_hours.append(name)\n",
    "\n",
    "    # 將列表轉換為 DataFrame\n",
    "    fully_represented_hours_df = pd.DataFrame(fully_represented_hours, columns=['date', 'hour'])\n",
    "\n",
    "    # 將 DataFrame 保存為 txt 文件\n",
    "    output_filename = filename.split('.')[0] + '_fully_represented_hours.txt'\n",
    "    fully_represented_hours_df.to_csv(output_filename, index=False, header=False, sep=' ')\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "filename = 'missing_datetimes.txt'\n",
    "output_filename = find_fully_represented_hours(filename)\n",
    "print(f\"所有小時內每10分鐘都有數據的時間點已保存在 {output_filename} 文件中\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data\n",
    "with open('datetime_strings.txt', 'r') as file:\n",
    "    datetime_strings = file.read().splitlines()\n",
    "\n",
    "# Define the function to subtract 8 hours from datetime\n",
    "def subtract_hours(dt_str):\n",
    "    dt_obj = datetime.strptime(dt_str, \"%Y%m%d %H\")\n",
    "    dt_obj = dt_obj - timedelta(hours=8)\n",
    "    return dt_obj.strftime(\"%Y%m%d %H\")\n",
    "\n",
    "# Apply the function to all datetime strings\n",
    "datetime_strings_adjusted = [subtract_hours(dt_str) for dt_str in datetime_strings]\n",
    "\n",
    "# Save the adjusted datetime strings back to a file\n",
    "with open('datetime_strings_adjusted.txt', 'w') as file:\n",
    "    for dt_str in datetime_strings_adjusted:\n",
    "        file.write(f\"{dt_str}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
